---
phase: 06-attention-optimization
plan: 02
type: execute
depends_on: ["06-01"]
files_modified: [src/attention/flash_attention.rs, src/attention/backend_registry.rs, src/attention/mod.rs]
autonomous: true
---

<objective>
Implement flash attention as a pluggable backend in the attention registry.

Purpose: Enable use of existing flash attention kernels through the backend registry system. This allows runtime selection between standard attention and flash attention backends.
Output: FlashAttention backend implementation registered and usable.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/06-attention-optimization/06-01-SUMMARY.md
@kernels/flash_attention.hip
@src/attention/backend_registry.rs
@src/attention/gpu.rs
@src/attention/kernels.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create FlashAttention backend implementation</name>
  <files>src/attention/flash_attention.rs</files>
  <action>
    Create src/attention/flash_attention.rs implementing BackendImplementation trait:

    Structure:
    ```rust
    use crate::attention::{
        backend_registry::{BackendImplementation, AttentionConfig, AttentionBackendResult},
        backend::AttentionBackend,
        kernel_tests::HipAttentionKernels,
    };
    use std::sync::Arc;

    pub struct FlashAttentionBackend {
        kernels: HipAttentionKernels,
    }

    impl FlashAttentionBackend {
        pub fn new(kernels: HipAttentionKernels) -> Self {
            Self { kernels }
        }
    }

    impl BackendImplementation for FlashAttentionBackend {
        fn name(&self) -> &str {
            "flash_attention"
        }

        fn supports(&self, config: &AttentionConfig) -> bool {
            // Support when:
            // - ROCm feature enabled
            // - Sequence length <= max_sequence_length
            // - Head dimensions compatible with shared memory
            cfg!(feature = "rocm") &&
            config.seq_len <= config.max_sequence_length &&
            config.head_dim <= 128  // Shared memory limit
        }

        fn required_kv_layout(&self) -> Option<KvCacheLayout> {
            // Flash attention requires specific KV cache layout
            Some(KvCacheLayout::Paged)
        }

        fn forward(
            &self,
            config: &AttentionConfig,
            q: &[f32],
            k: &[f32],
            v: &[f32],
            mask: Option<&[f32]>,
        ) -> AttentionBackendResult<Vec<f32>> {
            // Call appropriate kernel based on is_causal flag
            if config.is_causal {
                self.forward_causal(config, q, k, v, mask)
            } else {
                self.forward_nocausal(config, q, k, v, mask)
            }
        }
    }
    ```

    Implement forward_causal and forward_nocausal using kernels from kernels/flash_attention*.hip
  </action>
  <verify>src/attention/flash_attention.rs created with BackendImplementation trait</verify>
  <done>FlashAttention backend implemented</done>
</task>

<task type="auto">
  <name>Task 2: Register FlashAttention backend in registry</name>
  <files>src/attention/backend_registry.rs</files>
  <action>
    Update src/attention/backend_registry.rs to register FlashAttention backend:

    1. Import FlashAttentionBackend
    2. In init function or registration function, add:
       ```rust
       let flash_backend = FlashAttentionBackend::new(kernels.clone());
       registry.register("flash_attention".to_string(), Arc::new(flash_backend));
       ```

    3. Ensure backend is available when cfg(feature = "rocm")
  </action>
  <verify>FlashAttention backend registered in registry</verify>
  <done>Backend registration complete</done>
</task>

<task type="auto">
  <name>Task 3: Export FlashAttention module</name>
  <files>src/attention/mod.rs</files>
  <action>
    Add to src/attention/mod.rs:
    ```rust
    #[cfg(feature = "rocm")]
    pub mod flash_attention;
    #[cfg(feature = "rocm")]
    pub use flash_attention::FlashAttentionBackend;
    ```
  </action>
  <verify>flash_attention module exported</verify>
  <done>Module export complete</done>
</task>

<task type="auto">
  <name>Task 4: Add tests for FlashAttention backend</name>
<files>src/attention/flash_attention.rs</files>
  <action>
    Add tests in flash_attention.rs following pattern from other attention tests:

    Tests should cover:
    - Correctness vs reference CPU backend
    - Performance (should be faster for large models)
    - Causal vs non-causal variants
    - Edge cases (empty tensors, large inputs)

    Mark GPU-dependent tests with #[cfg(feature = "rocm")] and #[ignore]
  </action>
  <verify>Tests created and compile</verify>
  <done>Tests added</done>
</task>

</tasks>

<verification>
- [ ] src/attention/flash_attention.rs created with BackendImplementation
- [ ] FlashAttention backend registered in backend_registry.rs
- [ ] Module exported in mod.rs
- [ ] Tests created (may be GPU-gated)
- [ ] cargo check passes
</verification>

<success_criteria>
- FlashAttention backend implements BackendImplementation trait
- Backend registered and selectable
- Tests validate correctness vs reference backend
- Code follows established patterns from other backends
</success_criteria>

<output>
After completion, create `.planning/phases/06-attention-optimization/06-02-SUMMARY.md`
</output>
