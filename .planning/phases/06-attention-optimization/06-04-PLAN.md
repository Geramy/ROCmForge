---
phase: 06-attention-optimization
plan: 04
type: execute
depends_on: ["06-02", "06-03"]
files_modified: [benches/attention_bench.rs, tests/attention/README.md]
autonomous: true
---

<objective>
Benchmark flash attention performance and optimize based on measurements.

Purpose: Validate that flash attention provides performance improvement over standard attention and optimize critical bottlenecks. Establish baseline metrics for attention computation.
Output: Benchmark suite with baseline measurements and optimizations applied.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/06-attention-optimization/06-01-SUMMARY.md
@.planning/phases/06-attention-optimization/06-02-SUMMARY.md
@.planning/phases/06-attention-optimization/06-03-SUMMARY.md
@src/attention/flash_attention.rs
@src/attention/backend_registry.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create attention benchmark suite</name>
  <files>benches/attention_bench.rs</files>
  <action>
    Create benches/attention_bench.rs using Criterion:

    Benchmarks should cover:
    - Standard attention (CPU backend)
    - Flash attention (GPU backend)
    - Different sequence lengths (128, 256, 512, 1024, 2048)
    - Different head dimensions (32, 64, 128)
    - Causal vs non-causal
    - Varying batch sizes

    Criterion setup:
    ```rust
    use criterion::{black_box, criterion_group, criterion_main, Criterion};
    use crate::attention::{backend_registry::AttentionConfig, flash_attention::FlashAttentionBackend};
    ```

    Use criterion_group to organize related benchmarks.
  </action>
  <verify>benches/attention_bench.rs created with valid criterion benchmarks</verify>
  <done>Benchmark suite created</done>
</task>

<task type="auto">
  <name>Task 2: Create README for attention benchmarks</name>
  <files>tests/attention/README.md</files>
  <action>
    Create tests/attention/README.md documenting:

    - How to run benchmarks: `cargo bench --bench attention_bench`
    - Requirements: ROCm GPU, test model, environment setup
    - What each benchmark measures
    - Expected performance characteristics

    Include:
    - Hardware requirements (GPU memory, ROCm version)
    - Model requirements (test GGUF with attention layers)
    - Baseline expectations (CPU vs GPU)
    - How to interpret results
  </action>
  <verify>README created with benchmark instructions</verify>
  <done>README created with documentation</done>
</task>

<task type="auto">
  <name>Task 3: Run benchmarks and establish baselines</name>
  <files>benches/attention_bench.rs</files>
  <action>
    Run the benchmark suite and establish baseline metrics:

    ```bash
    cargo bench --bench attention_bench
    ```

    Document results in 06-04-SUMMARY.md:
    - Throughput (tokens/second)
    - Latency (time to first token)
    - Memory bandwidth utilization
    - CPU vs GPU comparison
    - Flash vs standard attention comparison

    If results show flash attention is slower than expected, investigate:
    - Kernel launch overhead
    - Memory bandwidth bottlenecks
    - Suboptimal block sizing
  </action>
  <verify>Benchmarks run successfully</verify>
  <done>Baselines established and documented</done>
</task>

<task type="auto">
  <name>Task 4: Implement optimizations based on profiling results</name>
  <files>src/attention/kernels.rs, src/attention/flash_attention.rs</files>
  <action>
    Based on benchmark results, apply optimizations:

    Potential optimizations:
    - Adjust block/grid sizing for better occupancy
    - Optimize memory access patterns for coalescing
    - Use shared memory more effectively
    - Tune algorithm parameters based on ROCm profiling tools

    Only implement optimizations with clear performance benefit.
    Document before/after measurements.
  </action>
  <verify>Optimizations applied and documented</verify>
  <done>Performance optimizations implemented</done>
</task>

</tasks>

<verification>
- [ ] Benchmark suite created with attention benchmarks
- [ ] README explains how to run and interpret benchmarks
- [ ] Baselines established from benchmark runs
- [ ] Optimizations applied if needed
</verification>

<success_criteria>
- Benchmark suite runs successfully
- Baseline metrics documented
- Clear path to optimization
- Flash attention performance measured
</success_criteria>

<output>
After completion, create `.planning/phases/06-attention-optimization/06-04-SUMMARY.md`
</output>
