---
phase: 06-attention-optimization
plan: 03
type: execute
depends_on: ["06-01", "06-02"]
files_modified: [build.rs, src/attention/kernels.rs]
autonomous: true
---

<objective>
Ensure flash attention HIP kernels are properly integrated into the build system and Rust wrapper functions.

Purpose: The flash attention kernels already exist (kernels/flash_attention.hip, flash_attention_causal.hip, flash_attention_nocausal.hip). This plan ensures they are compiled, wrapped, and accessible through the backend system.
Output: Build system integration for flash attention kernels with Rust wrapper functions in kernels.rs.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-ship-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/06-attention-optimization/06-01-SUMMARY.md
@.planning/phases/06-attention-optimization/06-02-SUMMARY.md
@kernels/flash_attention.hip
@build.rs
@src/attention/kernels.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify flash attention kernels in build system</name>
  <files>build.rs</files>
  <action>
    Check build.rs to ensure flash attention kernels are in the kernels array:

    Look for:
    ```rust
    (
        "kernels/flash_attention.hip",
        "FLASH_ATTENTION_HSACO",
        "flash_attention_kernel",
    ),
    (
        "kernels/flash_attention_causal.hip",
        "FLASH_ATTENTION_CAUSAL_HSACO",
        "flash_attention_causal_kernel",
    ),
    (
        "kernels/flash_attention_nocausal.hip",
        "FLASH_ATTENTION_NCAUSAL_HSACO",
        "flash_attention_nocausal_kernel",
    ),
    ```

    If not present, add them to the kernels array following the same pattern as other kernels.
  </action>
  <verify>All flash attention kernels in build.rs</verify>
  <done>Kernels verified in build system</done>
</task>

<task type="auto">
  <name>Task 2: Ensure Rust wrapper functions exist in kernels.rs</name>
  <files>src/attention/kernels.rs</files>
  <action>
    Check src/attention/kernels.rs for flash attention kernel wrappers:

    Look for:
    - External function declarations for HIP kernels
    - Wrapper functions that expose HIP kernels to Rust
    - Launch configuration (grid/block sizing)
    - Memory management for input/output buffers

    If wrappers exist, verify they're correct.
    If not, create them following pattern from other kernel wrappers in kernels.rs.
  </action>
  <verify>Flash attention wrappers exist in kernels.rs</verify>
  <done>Kernel wrappers verified/created</done>
</task>

<task type="auto">
  <name>Task 3: Ensure FlashAttentionBackend calls flash kernels correctly</name>
  <files>src/attention/flash_attention.rs</files>
  <action>
    Verify src/attention/flash_attention.rs (created in 06-02) properly calls the kernel wrappers.

    Check that:
    - forward_causal calls flash_attention_causal_kernel
    - forward_nocausal calls flash_attention_nocausal_kernel
    - Grid/block sizing matches kernel expectations
    - Memory layouts are correct (row-major, specific strides)

    Adjust if needed to match kernel interface.
  </action>
  <verify>FlashAttentionBackend correctly calls HIP kernels</verify>
  <done>Kernel calls verified</done>
</task>

<task type="auto">
  <name>Task 4: Add integration tests for flash attention</name>
<files>src/attention/flash_attention_tests.rs</files>
  <action>
    Create or update flash_attention_tests.rs with integration tests:

    Test scenarios:
    - FlashAttention backend selectable from registry
    - Correctness vs reference backend (small inputs)
    - Performance comparison (large inputs, if GPU available)
    - Causal vs non-causal correctness
    - Error handling for unsupported configurations

    Follow pattern from existing attention tests (causal_mask_tests.rs, flash_attention_tests.rs).
  </action>
  <verify>Integration tests created and passing</verify>
  <done>Integration tests added</done>
</task>

</tasks>

<verification>
- [ ] All flash attention kernels in build.rs
- [ ] Rust wrapper functions exist in kernels.rs
- [ ] FlashAttentionBackend correctly calls kernels
- [ ] Integration tests pass
- [ ] cargo check passes
</verification>

<success_criteria>
- Flash attention kernels compile and are accessible from Rust
- FlashAttentionBackend functional and selectable
- Tests validate correctness
- Ready for benchmarking in 06-04
</success_criteria>

<output>
After completion, create `.planning/phases/06-attention-optimization/06-03-SUMMARY.md`
</output>
