---
phase: 06-attention-optimization
plan: 01
type: execute
depends_on: []
files_modified: [.planning/phases/06-attention-optimization/RESEARCH.md]
autonomous: true
---

<objective>
Research existing flash attention implementation and document integration strategy.

Purpose: Understand what flash attention kernels already exist, how they work, and what needs to be done to integrate them into the attention backend registry.
Output: Research documentation with integration strategy for connecting flash attention kernels to the backend system.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@kernels/flash_attention.hip
@kernels/flash_attention_causal.hip
@kernels/flash_attention_nocausal.hip
@src/attention/backend_registry.rs
@src/attention/backend.rs
@src/attention/gpu.rs
@src/attention/kernels.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Document existing flash attention kernels</name>
  <files>.planning/phases/06-attention-optimization/RESEARCH.md</files>
  <action>
    Create RESEARCH.md documenting all existing flash attention kernels:

    For each kernel (flash_attention, flash_attention_causal, flash_attention_nocausal), document:
    - Kernel algorithm and optimization approach
    - Input/output tensor layouts
    - Block and grid sizing for RDNA3
    - Causal vs non-causal variants
    - Integration points with existing code

    Include:
    - Kernel specifications from kernel comments
    - How kernels map to attention computation (QK^T, softmax, weighted sum)
    - Memory access patterns and optimization techniques
  </action>
  <verify>RESEARCH.md created with kernel documentation</verify>
  <done>Existing flash attention kernels documented</done>
</task>

<task type="auto">
  <name>Task 2: Analyze attention backend registry and GPU backend</name>
  <files>.planning/phases/06-attention-optimization/RESEARCH.md</files>
  <action>
    Analyze how existing backends work:

    From src/attention/backend_registry.rs:
    - BackendImplementation trait interface
    - AttentionConfig structure
    - RegisterBackend mechanism

    From src/attention/gpu.rs:
    - Current GPU backend implementation
    - How it wraps hip kernels
    - Integration pattern

    Document:
    - How flash attention kernels differ from current GPU backend
    - What changes needed to register FlashAttention backend
    - Whether flash attention should be a separate backend or optimization variant
  </action>
  <verify>Backend registry analysis documented in RESEARCH.md</verify>
  <done>Backend registry analysis complete</done>
</task>

<task type="auto">
  <name>Task 3: Document flash attention paper and algorithm</name>
  <files>.planning/phases/06-attention-optimization/RESEARCH.md</files>
  <action>
    Document the flash attention algorithm based on kernel code:

    From kernels/flash_attention.hip:
    - Fused operation: QK^T + scale + mask + softmax + softmax*V in single kernel
    - Eliminates CPU-GPU round-trips
    - Block-based computation (256 threads per block)
    - Causal mask support

    Include:
    - How it achieves speedup (memory coalescing, kernel fusion)
    - Tradeoffs vs traditional attention (limited by shared memory)
    - ROCm/AMDGPU specific optimizations (RDNA3 wave32)
  </action>
  <verify>Flash attention algorithm documented</verify>
  <done>Algorithm analysis complete</done>
</task>

<task type="auto">
  <name>Task 4: Design integration strategy</name>
  <files>.planning/phases/06-attention-optimization/RESEARCH.md</files>
  <action>
    Design integration strategy:

    Options:
    1. Separate FlashAttention backend - Register as new backend implementation
    2. Optimization variant of GPU backend - Detect when flash attention can be used and switch kernels
    3. Automatic detection - Use flash attention when conditions met (seq_len, head_dim, hardware support)

    For each option, document:
    - Implementation complexity
    - Performance implications
    - Integration points
    - Pros/cons

    Recommend approach based on:
    - Code simplicity
    - Performance gain
    - User experience
  </action>
  <verify>Integration strategy designed and documented</verify>
  <done>Integration strategy defined</done>
</task>

</tasks>

<verification>
- [ ] RESEARCH.md created with comprehensive documentation
- [ ] All existing flash attention kernels documented
- [ ] Backend registry analysis complete
- [ ] Integration strategy defined
</verification>

<success_criteria>
- Clear understanding of existing flash attention implementation
- Clear path from research to implementation
- Integration strategy balances performance with code complexity
- Ready for implementation in 06-02 and 06-03
</success_criteria>

<output>
After completion, create `.planning/phases/06-attention-optimization/06-01-SUMMARY.md`
</output>
