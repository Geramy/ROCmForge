---
phase: 07-hybrid-execution
plan: 01
type: execute
depends_on: []
files_modified: [src/ggml/hybrid_scheduler.rs, src/ggml/mod.rs]
autonomous: true
---

<objective>
Design hybrid execution scheduler architecture for automatic CPU/GPU operation selection.

Purpose: Create the architecture for runtime selection between CPU and GPU backends based on operation availability, hardware capabilities, and cost modeling. This enables transparent fallback when GPU operations aren't available.
Output: HybridScheduler module with architecture and trait definitions.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/checkpoints.md
@~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/ggml/backend.rs
@src/ggml/cpu_backend.rs
@src/ggml/hip_backend/mod.rs
@src/ggml/executor.rs
@src/ggml/op.rs

@.planning/phases/04-cpu-simd-backend/04-04-SUMMARY.md
@.planning/phases/05-quantized-operations/05-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create operation capability tracking trait</name>
  <files>src/ggml/hybrid_scheduler.rs</files>
  <action>
    Create src/ggml/hybrid_scheduler.rs with the capability tracking trait:

    ```rust
    //! Hybrid execution scheduler for automatic CPU/GPU operation selection

    use crate::ggml::{GgmlBackend, GgmlResult, Op};
    use std::collections::HashSet;

    /// Capability descriptor for a backend operation
    #[derive(Debug, Clone, PartialEq, Eq, Hash)]
    pub struct OpCapability {
        pub op_type: OpType,
        pub supported_dtypes: Vec<DType>,
        pub max_tensor_size: Option<usize>,
        pub requires_feature: Option<String>, // e.g., "rocm", "simd"
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
    pub enum OpType {
        MatMul,
        QuantizedMatMul,
        Add,
        Scale,
        Softmax,
        Attention,
        Dequantize,
        // Add more as needed
    }

    /// Trait for backends that declare their operation capabilities
    pub trait CapableBackend: GgmlBackend {
        /// Get all operations this backend can execute
        fn capabilities(&self) -> HashSet<OpCapability>;

        /// Check if this backend can execute a specific operation
        fn can_execute(&self, op: &Op) -> bool {
            let cap = self.op_capability(op);
            cap.map_or(false, |c| self.capabilities().contains(&c))
        }

        /// Get the capability requirement for an operation
        fn op_capability(&self, op: &Op) -> Option<OpCapability>;
    }

    /// Cost estimate for executing an operation on a backend
    #[derive(Debug, Clone, Copy)]
    pub struct OpCost {
        pub estimated_us: u64,        // Estimated execution time in microseconds
        pub memory_bytes: usize,       // Estimated memory usage
        pub transfer_cost: Option<u64>, // Data transfer cost (for heterogeneous execution)
    }
    ```

    This trait allows backends to declare what they can do, enabling the scheduler to make informed decisions.
  </action>
  <verify>src/ggml/hybrid_scheduler.rs created with capability tracking trait</verify>
  <done>Capability tracking trait defined</done>
</task>

<task type="auto">
  <name>Task 2: Create HybridScheduler struct with basic architecture</name>
  <files>src/ggml/hybrid_scheduler.rs</files>
  <action>
    Add to hybrid_scheduler.rs:

    ```rust
    use std::sync::Arc;

    /// Execution strategy for operation scheduling
    #[derive(Debug, Clone, Copy)]
    pub enum ExecutionStrategy {
        /// Always use GPU if available, fallback to CPU
        GpuPreferred,
        /// Always use CPU if available
        CpuPreferred,
        /// Automatically select based on cost model
        Automatic,
        /// Use specified backend only
        BackendOnly(&'static str),
    }

    /// Selection reason for telemetry
    #[derive(Debug, Clone)]
    pub enum SelectionReason {
        GpuAvailable,
        GpuUnavailable { reason: String },
        CpuFallback,
        CostModel { gpu_cost: OpCost, cpu_cost: OpCost },
        MemoryConstraint { required: usize, available: usize },
        UserPreference(String),
    }

    /// Backend selection result
    #[derive(Debug, Clone)]
    pub struct BackendSelection {
        pub backend_id: String,
        pub reason: SelectionReason,
        pub estimated_cost: OpCost,
    }

    /// Hybrid execution scheduler
    pub struct HybridScheduler {
        cpu_backend: Option<Arc<dyn CapableBackend>>,
        gpu_backend: Option<Arc<dyn CapableBackend>>,
        strategy: ExecutionStrategy,
        telemetry: Vec<ExecutionEvent>,
    }

    impl HybridScheduler {
        pub fn new(strategy: ExecutionStrategy) -> Self {
            Self {
                cpu_backend: None,
                gpu_backend: None,
                strategy,
                telemetry: Vec::new(),
            }
        }

        pub fn with_cpu_backend(mut self, backend: Arc<dyn CapableBackend>) -> Self {
            self.cpu_backend = Some(backend);
            self
        }

        pub fn with_gpu_backend(mut self, backend: Arc<dyn CapableBackend>) -> Self {
            self.gpu_backend = Some(backend);
            self
        }

        /// Select the best backend for executing an operation
        pub fn select_backend(&self, op: &Op) -> GgmlResult<BackendSelection> {
            match self.strategy {
                ExecutionStrategy::GpuPreferred => self.select_gpu_preferred(op),
                ExecutionStrategy::CpuPreferred => self.select_cpu_preferred(op),
                ExecutionStrategy::Automatic => self.select_automatic(op),
                ExecutionStrategy::BackendOnly(name) => self.select_named(name, op),
            }
        }

        fn select_gpu_preferred(&self, op: &Op) -> GgmlResult<BackendSelection> {
            if let Some(gpu) = &self.gpu_backend {
                if gpu.can_execute(op) {
                    return Ok(BackendSelection {
                        backend_id: "gpu".to_string(),
                        reason: SelectionReason::GpuAvailable,
                        estimated_cost: self.estimate_cost(gpu, op),
                    });
                }
                return Ok(BackendSelection {
                    backend_id: "gpu".to_string(),
                    reason: SelectionReason::GpuUnavailable {
                        reason: "Operation not supported".to_string(),
                    },
                    estimated_cost: OpCost { estimated_us: 0, memory_bytes: 0, transfer_cost: None },
                });
            }
            Err("No GPU backend available".into())
        }

        fn select_cpu_preferred(&self, op: &Op) -> GgmlResult<BackendSelection> {
            if let Some(cpu) = &self.cpu_backend {
                if cpu.can_execute(op) {
                    return Ok(BackendSelection {
                        backend_id: "cpu".to_string(),
                        reason: SelectionReason::CpuFallback,
                        estimated_cost: self.estimate_cost(cpu, op),
                    });
                }
            }
            Err("No CPU backend available".into())
        }

        fn select_automatic(&self, op: &Op) -> GgmlResult<BackendSelection> {
            // For now, prefer GPU if available and supports the operation
            // Cost modeling will be enhanced in 07-03
            let gpu_available = self.gpu_backend.as_ref()
                .map(|b| b.can_execute(op))
                .unwrap_or(false);

            if gpu_available {
                self.select_gpu_preferred(op)
            } else if let Some(cpu) = &self.cpu_backend {
                if cpu.can_execute(op) {
                    Ok(BackendSelection {
                        backend_id: "cpu".to_string(),
                        reason: SelectionReason::CpuFallback,
                        estimated_cost: self.estimate_cost(cpu, op),
                    })
                } else {
                    Err("No backend can execute this operation".into())
                }
            } else {
                Err("No backends available".into())
            }
        }

        fn select_named(&self, name: &str, op: &Op) -> GgmlResult<BackendSelection> {
            match name {
                "gpu" => self.select_gpu_preferred(op),
                "cpu" => self.select_cpu_preferred(op),
                _ => Err(format!("Unknown backend: {}", name).into()),
            }
        }

        fn estimate_cost(&self, backend: &dyn CapableBackend, op: &Op) -> OpCost {
            // Basic cost estimation - will be enhanced in 07-03
            OpCost {
                estimated_us: 100, // Placeholder
                memory_bytes: 1024,
                transfer_cost: None,
            }
        }
    }
    ```

    This provides the core scheduler architecture with multiple execution strategies.
  </action>
  <verify>HybridScheduler struct created with execution strategies</verify>
  <done>Scheduler architecture complete</done>
</task>

<task type="auto">
  <name>Task 3: Add telemetry event types</name>
  <files>src/ggml/hybrid_scheduler.rs</files>
  <action>
    Add telemetry types to track execution decisions:

    ```rust
    use std::time::Instant;

    /// Telemetry event for execution tracking
    #[derive(Debug, Clone)]
    pub struct ExecutionEvent {
        pub timestamp: Instant,
        pub operation: OpType,
        pub backend: String,
        pub reason: SelectionReason,
        pub actual_duration_us: Option<u64>,
    }

    impl HybridScheduler {
        pub fn record_execution(&mut self, event: ExecutionEvent) {
            self.telemetry.push(event);
        }

        pub fn get_telemetry(&self) -> &[ExecutionEvent] {
            &self.telemetry
        }

        pub fn clear_telemetry(&mut self) {
            self.telemetry.clear();
        }

        /// Get statistics about backend usage
        pub fn backend_usage_stats(&self) -> BackendStats {
            let mut gpu_count = 0;
            let mut cpu_count = 0;

            for event in &self.telemetry {
                match event.backend.as_str() {
                    "gpu" => gpu_count += 1,
                    "cpu" => cpu_count += 1,
                    _ => {}
                }
            }

            BackendStats {
                total_operations: self.telemetry.len(),
                gpu_operations: gpu_count,
                cpu_operations: cpu_count,
            }
        }
    }

    #[derive(Debug, Clone)]
    pub struct BackendStats {
        pub total_operations: usize,
        pub gpu_operations: usize,
        pub cpu_operations: usize,
    }
    ```

    Telemetry enables debugging and optimization of scheduler decisions.
  </action>
  <verify>Telemetry types added to hybrid_scheduler.rs</verify>
  <done>Telemetry system defined</done>
</task>

<task type="auto">
  <name>Task 4: Export hybrid_scheduler module</name>
  <files>src/ggml/mod.rs</files>
  <action>
    Add to src/ggml/mod.rs:

    ```rust
    pub mod hybrid_scheduler;
    pub use hybrid_scheduler::{
        CapableBackend, HybridScheduler, ExecutionStrategy,
        OpCapability, OpType, OpCost, BackendSelection, SelectionReason,
        ExecutionEvent, BackendStats,
    };
    ```

    Also add basic tests in hybrid_scheduler.rs:

    ```rust
    #[cfg(test)]
    mod tests {
        use super::*;

        #[test]
        fn test_execution_strategy_variants() {
            // Verify all strategy variants can be created
            let _ = ExecutionStrategy::GpuPreferred;
            let _ = ExecutionStrategy::CpuPreferred;
            let _ = ExecutionStrategy::Automatic;
            let _ = ExecutionStrategy::BackendOnly("cpu");
        }

        #[test]
        fn test_scheduler_creation() {
            let scheduler = HybridScheduler::new(ExecutionStrategy::Automatic);
            let stats = scheduler.backend_usage_stats();
            assert_eq!(stats.total_operations, 0);
            assert_eq!(stats.gpu_operations, 0);
            assert_eq!(stats.cpu_operations, 0);
        }

        #[test]
        fn test_telemetry_tracking() {
            let mut scheduler = HybridScheduler::new(ExecutionStrategy::GpuPreferred);
            let event = ExecutionEvent {
                timestamp: Instant::now(),
                operation: OpType::MatMul,
                backend: "cpu".to_string(),
                reason: SelectionReason::CpuFallback,
                actual_duration_us: Some(100),
            };
            scheduler.record_execution(event);
            assert_eq!(scheduler.get_telemetry().len(), 1);
        }
    }
    ```
  </action>
  <verify>Module exported and tests compile</verify>
  <done>Module exported with tests</done>
</task>

</tasks>

<verification>
- [ ] src/ggml/hybrid_scheduler.rs created with ~250 LOC
- [ ] CapableBackend trait defined
- [ ] HybridScheduler with execution strategies implemented
- [ ] Telemetry types added
- [ ] Module exported in mod.rs
- [ ] Tests compile
</verification>

<success_criteria>
- Capability tracking trait allows backends to declare supported operations
- HybridScheduler supports multiple execution strategies (GpuPreferred, CpuPreferred, Automatic, BackendOnly)
- Telemetry system tracks execution decisions
- Tests validate basic functionality
- Code follows established patterns from GgmlBackend trait
</success_criteria>

<output>
After completion, create `.planning/phases/07-hybrid-execution/07-01-SUMMARY.md`
</output>
