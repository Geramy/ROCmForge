---
phase: 07-hybrid-execution
plan: 03
type: execute
depends_on: ["07-01", "07-02"]
files_modified: [src/ggml/hybrid_scheduler.rs, src/ggml/executor.rs]
autonomous: true
---

<objective>
Implement automatic operation selection based on backend availability and cost model.

Purpose: Enhance the HybridScheduler to automatically select the best backend for each operation based on capabilities, estimated costs, and execution strategy. Integrate with the graph executor to use the scheduler.
Output: Working automatic backend selection integrated into graph execution.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/checkpoints.md
@~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/07-hybrid-execution/07-01-SUMMARY.md
@.planning/phases/07-hybrid-execution/07-02-SUMMARY.md

@src/ggml/hybrid_scheduler.rs
@src/ggml/executor.rs
@src/ggml/cpu_backend.rs
@src/ggml/hip_backend/backend.rs
@src/ggml/op.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement enhanced cost estimation in HybridScheduler</name>
  <files>src/ggml/hybrid_scheduler.rs</files>
  <action>
    Add enhanced cost estimation to hybrid_scheduler.rs:

    ```rust
    impl HybridScheduler {
        /// Enhanced cost estimation based on operation properties
        fn estimate_cost(&self, backend: &dyn CapableBackend, op: &Op) -> OpCost {
            // Get tensor sizes from the operation
            let tensor_elements = self.estimate_tensor_elements(op);
            let is_gpu = backend.capabilities().iter()
                .any(|c| c.requires_feature.as_deref() == Some("rocm"));

            // Base estimates (in microseconds)
            let base_us = match OpType::from_op(op) {
                Some(OpType::MatMul) => 10,
                Some(OpType::QuantizedMatMul) => 5,  // Faster due to fusion
                Some(OpType::Softmax) => 5,
                Some(OpType::Add) => 1,
                Some(OpType::Scale) => 1,
                Some(OpType::Attention) => 20,
                Some(OpType::Dequantize) => 2,
                None => return OpCost { estimated_us: 0, memory_bytes: 0, transfer_cost: None },
            };

            // Scale by tensor size (logarithmic scaling)
            let size_factor = (tensor_elements as f64).log2().max(1.0) as u64;
            let estimated_us = base_us * size_factor;

            // Memory estimate (4 bytes per element for F32)
            let memory_bytes = tensor_elements * 4;

            // Transfer cost for heterogeneous execution
            let transfer_cost = if is_gpu {
                None  // GPU already has data
            } else {
                Some(estimated_us / 10)  // 10% overhead for PCIe transfer
            };

            OpCost {
                estimated_us,
                memory_bytes,
                transfer_cost,
            }
        }

        /// Estimate total number of tensor elements for an operation
        fn estimate_tensor_elements(&self, op: &Op) -> usize {
            // Extract tensor sizes from operation
            match op {
                Op::MatMul { a, b } => {
                    // Assume row-major: output is (M x N) where A is (M x K), B is (K x N)
                    2048  // Default estimate if we can't inspect tensors
                }
                Op::Softmax { input } => 1024,
                Op::Add { a, b } => 1024,
                Op::Scale { x, .. } => 1024,
                Op::QuantizedMatMul { .. } => 2048,
                Op::Attention { .. } => 4096,
                _ => 1024,
            }
        }
    }
    ```

    The cost model considers operation type, tensor size, and data transfer overhead.
  </action>
  <verify>Enhanced cost estimation implemented</verify>
  <done>Cost model based on operation properties</done>
</task>

<task type="auto">
  <name>Task 2: Implement automatic selection with cost comparison</name>
  <files>src/ggml/hybrid_scheduler.rs</files>
  <action>
    Replace the select_automatic method with cost-based comparison:

    ```rust
    impl HybridScheduler {
        fn select_automatic(&self, op: &Op) -> GgmlResult<BackendSelection> {
            let gpu_available = self.gpu_backend.as_ref()
                .map(|b| b.can_execute(op))
                .unwrap_or(false);

            let cpu_available = self.cpu_backend.as_ref()
                .map(|b| b.can_execute(op))
                .unwrap_or(false);

            match (gpu_available, cpu_available) {
                (true, true) => {
                    // Both available - compare costs
                    if let (Some(gpu), Some(cpu)) = (&self.gpu_backend, &self.cpu_backend) {
                        let gpu_cost = self.estimate_cost(gpu.as_ref(), op);
                        let cpu_cost = self.estimate_cost(cpu.as_ref(), op);

                        // Prefer GPU unless CPU is significantly faster
                        // (e.g., for very small operations where transfer cost dominates)
                        if cpu_cost.estimated_us < gpu_cost.estimated_us / 2 {
                            // CPU is at least 2x faster - use it
                            Ok(BackendSelection {
                                backend_id: "cpu".to_string(),
                                reason: SelectionReason::CostModel {
                                    gpu_cost,
                                    cpu_cost,
                                },
                                estimated_cost: cpu_cost,
                            })
                        } else {
                            Ok(BackendSelection {
                                backend_id: "gpu".to_string(),
                                reason: SelectionReason::CostModel {
                                    gpu_cost,
                                    cpu_cost,
                                },
                                estimated_cost: gpu_cost,
                            })
                        }
                    } else {
                        unreachable!()
                    }
                }
                (true, false) => self.select_gpu_preferred(op),
                (false, true) => self.select_cpu_preferred(op),
                (false, false) => {
                    Err("No backend can execute this operation".into())
                }
            }
        }
    }
    ```

    The automatic mode now compares estimated costs and chooses the fastest backend.
  </action>
  <verify>Cost-based selection implemented</verify>
  <done>Automatic mode uses cost model</done>
</task>

<task type="auto">
  <name>Task 3: Create HybridExecutor that wraps backends</name>
  <files>src/ggml/hybrid_scheduler.rs</files>
  <action>
    Add HybridExecutor that implements GgmlBackend and delegates to selected backends:

    ```rust
    use crate::ggml::{GgmlBackend, TensorDesc, TensorId};
    use std::collections::HashMap;

    /// Hybrid executor that delegates to CPU or GPU backend based on scheduler decisions
    pub struct HybridExecutor {
        scheduler: HybridScheduler,
        cpu_backend: Option<Box<dyn GgmlBackend>>,
        gpu_backend: Option<Box<dyn GgmlBackend>>,
        active_backend: Option<String>,  // Track which backend is currently active
    }

    impl HybridExecutor {
        pub fn new(cpu: Box<dyn GgmlBackend>, gpu: Option<Box<dyn GgmlBackend>>) -> Self {
            let strategy = if gpu.is_some() {
                ExecutionStrategy::Automatic
            } else {
                ExecutionStrategy::CpuPreferred
            };

            let mut scheduler = HybridScheduler::new(strategy);

            // Note: We can't register as CapableBackend here since GgmlBackend doesn't
            // require that trait. The scheduler is used for selection, and we
            // delegate directly to the wrapped backends for execution.

            Self {
                scheduler,
                cpu_backend: Some(cpu),
                gpu_backend: gpu,
                active_backend: None,
            }
        }

        /// Get the scheduler for configuration
        pub fn scheduler(&self) -> &HybridScheduler {
            &self.scheduler
        }

        /// Get mutable scheduler for configuration
        pub fn scheduler_mut(&mut self) -> &mut HybridScheduler {
            &mut self.scheduler
        }

        /// Select backend for an operation (returns "cpu" or "gpu")
        fn select_backend_for_op(&self, op: &Op) -> GgmlResult<String> {
            // Simple heuristic based on operation type
            // In a full implementation, this would use the scheduler's cost model
            if self.gpu_backend.is_some() {
                match op {
                    Op::MatMul { .. } | Op::QuantizedMatMul { .. } | Op::Attention { .. } => {
                        Ok("gpu".to_string())
                    }
                    _ => Ok("cpu".to_string()),
                }
            } else {
                Ok("cpu".to_string())
            }
        }

        fn get_backend(&self, name: &str) -> GgmlResult<&dyn GgmlBackend> {
            match name {
                "cpu" => self.cpu_backend.as_ref()
                    .map(|b| b.as_ref())
                    .ok_or("CPU backend not available".into()),
                "gpu" => self.gpu_backend.as_ref()
                    .map(|b| b.as_ref())
                    .ok_or("GPU backend not available".into()),
                _ => Err(format!("Unknown backend: {}", name).into()),
            }
        }
    }

    impl GgmlBackend for HybridExecutor {
        type Buffer = Box<dyn std::any::Any>;

        fn alloc(&mut self, desc: &TensorDesc) -> GgmlResult<()> {
            // Allocate on CPU for simplicity
            if let Some(cpu) = self.cpu_backend.as_mut() {
                cpu.alloc(desc)
            } else {
                Err("No backend available for allocation".into())
            }
        }

        fn bind(&mut self, desc: &TensorDesc, buffer: Self::Buffer) -> GgmlResult<()> {
            if let Some(cpu) = self.cpu_backend.as_mut() {
                cpu.bind(desc, buffer)
            } else {
                Err("No backend available".into())
            }
        }

        fn free(&mut self, id: TensorId) -> GgmlResult<()> {
            if let Some(cpu) = self.cpu_backend.as_mut() {
                cpu.free(id)
            } else {
                Err("No backend available".into())
            }
        }

        fn tensor_desc(&self, id: TensorId) -> Option<&TensorDesc> {
            self.cpu_backend.as_ref()?.tensor_desc(id)
        }

        fn buffer(&self, id: TensorId) -> Option<&Self::Buffer> {
            self.cpu_backend.as_ref()?.buffer(id)
        }

        fn buffer_mut(&mut self, id: TensorId) -> Option<&mut Self::Buffer> {
            self.cpu_backend.as_mut()?.buffer_mut(id)
        }

        fn execute_op(
            &mut self,
            op: &Op,
            inputs: &[TensorId],
            outputs: &[TensorId],
        ) -> GgmlResult<()> {
            // Select backend for this operation
            let backend_name = self.select_backend_for_op(op)?;
            self.active_backend = Some(backend_name.clone());

            let backend = self.get_backend(&backend_name)?;
            backend.execute_op(op, inputs, outputs)
        }

        fn synchronize(&mut self) -> GgmlResult<()> {
            // Synchronize all backends
            if let Some(cpu) = self.cpu_backend.as_mut() {
                cpu.synchronize()?;
            }
            if let Some(gpu) = self.gpu_backend.as_mut() {
                gpu.synchronize()?;
            }
            Ok(())
        }
    }
    ```

    The HybridExecutor delegates operations to the appropriate backend.
  </action>
  <verify>HybridExecutor implements GgmlBackend</verify>
  <done>Delegating executor created</done>
</task>

<task type="auto">
  <name>Task 4: Add tests for automatic selection</name>
  <files>src/ggml/hybrid_scheduler.rs</files>
  <action>
    Add automatic selection tests:

    ```rust
    #[cfg(test)]
    mod automatic_selection_tests {
        use super::*;
        use crate::ggml::TensorId;

        #[test]
        fn test_automatic_prefers_gpu_for_large_ops() {
            let mut scheduler = HybridScheduler::new(ExecutionStrategy::Automatic);
            // Would need mock backends to fully test
            // For now, verify the method exists
            let op = Op::MatMul {
                a: TensorId::new(0),
                b: TensorId::new(1),
            };
            // Without backends, should error
            assert!(scheduler.select_backend(&op).is_err());
        }

        #[test]
        fn test_cost_comparison() {
            let gpu_cost = OpCost {
                estimated_us: 100,
                memory_bytes: 1024,
                transfer_cost: None,
            };
            let cpu_cost = OpCost {
                estimated_us: 40,  // CPU is faster
                memory_bytes: 1024,
                transfer_cost: None,
            };

            // CPU is more than 2x faster - should be preferred
            assert!(cpu_cost.estimated_us < gpu_cost.estimated_us / 2);
        }
    }
    ```
  </action>
  <verify>Tests for automatic selection added</verify>
  <done>Automatic selection tests passing</done>
</task>

</tasks>

<verification>
- [ ] Enhanced cost estimation based on operation type and tensor size
- [ ] Automatic selection compares costs between backends
- [ ] HybridExecutor delegates to appropriate backend
- [ ] Tests validate selection logic
- [ ] cargo check passes
</verification>

<success_criteria>
- Scheduler automatically selects best backend based on cost model
- HybridExecutor transparently delegates operations
- Small operations may use CPU to avoid transfer overhead
- Large operations prefer GPU for performance
- Tests verify correct behavior
</success_criteria>

<output>
After completion, create `.planning/phases/07-hybrid-execution/07-03-SUMMARY.md`
</output>
