---
phase: 04-cpu-simd-backend
plan: 03
type: execute
depends_on: ["04-02"]
files_modified: [src/attention/cpu.rs]
autonomous: true
---

<objective>
Implement SIMD for attention operations for optimized CPU fallback.

Purpose: Accelerate attention computation (softmax, QK^T, weighted value) using SIMD
Output: Working SIMD-optimized attention operations
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/04-cpu-simd-backend/04-02-PLAN.md
@src/attention/cpu.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement softmax with SIMD</name>
  <files>src/attention/cpu.rs</files>
  <action>
    Implement softmax function using packed_simd:

    ```rust
    use packed_simd::{f32x4, m32x8};

    pub fn softmax_simd(logits: &[f32]) -> Vec<f32> {
        // Use SIMD to compute exp(logits - max) efficiently
        let max_val = logits.iter().fold(f32::NEG_INFINITY, |a, b| a.max(b));

        // Process 4 floats at a time
        let mut exp_vals = vec![0.0; logits.len()];
        for chunk in logits.chunks(4) {
            if chunk.len() == 4 {
                let vec = f32x4::from_slice_unaligned(chunk);
                let max_vec = f32x4::splat(max_val);
                let exp_vec = (vec - max_vec).exp();
                let exp_arr = exp_val.toArray();
                exp_vals[chunk.start()..chunk.start() + 4].copy_from_slice(&exp_arr);
            } else {
                // Handle remaining elements
                for (i, &val) in chunk.iter().enumerate() {
                    exp_vals[chunk.start() + i] = (val - max_val).exp();
                }
            }
        }

        // Compute sum for softmax
        let sum: f32 = exp_vals.iter().sum();
        let inv_sum = 1.0 / sum;

        // Normalize
        for val in exp_vals.iter_mut() {
            *val *= inv_sum;
        }

        exp_vals
    }
    ```

    Key optimizations:
    - Vectorized exp computation
    - Max computation using horizontal operations
    - Fallback for non-4-length chunks
  </action>
  <verify>cargo check passes</verify>
  <done>SIMD softmax implemented</done>
</task>

<task type="auto">
  <name>Task 2: Implement QK^T with SIMD</name>
  <files>src/attention/cpu.rs</files>
  <action>
    Implement query-key transpose operation using packed_simd:

    ```rust
    use packed_simd::{f32x4, m32x8};

    pub fn qk_t_simd(q: &[f32], k: &[f32], n: usize) -> Vec<f32> {
        // Q: [n×k], K: [k×n], output: [n×n]
        let mut result = vec![0.0; n * n];
        let tile_size = 4;

        for i in (0..n).step_by(tile_size) {
            for j in (0..n).step_by(tile_size) {
                let mut sum = f32x4::splat(0.0);

                for kk in (0..k).step_by(tile_size) {
                    let q_vec = f32x4::from_slice_unaligned(&q[i * k + kk..]);
                    let k_vec = f32x4::from_slice_unaligned(&k[kk * n + j..]);

                    sum += q_vec * k_vec;
                }

                // Store result
                let result_vec = sum.toArray();
                for jj in 0..tile_size.min(n - j) {
                    result[i * n + j + jj] = result_vec[jj];
                }
            }
        }

        Ok(result)
    }
    ```

    Notes:
    - This is the core attention operation
    - Tile size of 4 matches f32x4 vector width
    - Handles remaining elements with scalar fallback
  </action>
  <verify>cargo check passes</verify>
  <done>SIMD QK^T implemented</done>
</task>

<task type="auto">
  <name>Task 3: Implement weighted value with SIMD</name>
  <files>src/attention/cpu.rs</files>
  <action>
    Implement weighted value operation using packed_simd:

    ```rust
    use packed_simd::{f32x4, m32x8};

    pub fn weighted_value_simd(
        value: &[f32],
        weight: &[f32],
        output: &mut [f32],
    ) -> GgmlResult<()> {
        // value: [n], weight: [n], output: [n]
        // Perform output[i] = value[i] * weight[i] using SIMD

        let n = value.len();
        let tile_size = 4;

        for i in (0..n).step_by(tile_size) {
            if i + tile_size <= n {
                let v_vec = f32x4::from_slice_unaligned(&value[i..i + tile_size]);
                let w_vec = f32x4::from_slice_unaligned(&weight[i..i + tile_size]);
                let result_vec = v_vec * w_vec;

                let result_arr = result_vec.toArray();
                output[i..i + tile_size].copy_from_slice(&result_arr);
            } else {
                // Handle remaining elements
                for (idx, (v, w)) in value[i..].iter().enumerate() {
                    output[idx] = v * w;
                }
            }
        }

        Ok(())
    }
    ```

    This operation is used for attention weight application.
  </action>
  <verify>cargo check passes</verify>
  <done>SIMD weighted value implemented</done>
</task>

</tasks>

<verification>
- [ ] softmax_simd implemented with packed_simd
- [ ] qk_t_simd implemented with packed_simd
- [] weighted_value_simd implemented with packed_simd
- [ ] cargo check passes
- [ | All attention CPU tests pass
</verification>

<success_criteria>
- Attention operations use packed_simd for acceleration
- Scalar fallback for edge cases
- No breaking changes to attention API
</success_criteria>

<output>
After completion, create `.planning/phases/04-cpu-simd-backend/04-03-SUMMARY.md`
</output>
